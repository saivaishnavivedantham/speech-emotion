## ğŸ™ï¸ Speech Emotion Recognition
**ğŸ“ Institution:** UTA  

Developed a machine learning pipeline to detect emotions from speech using audio features and classical ML models.

### âœ¨ Highlights
- Used **librosa** to extract **MFCC** features from the **TESS** (Toronto Emotional Speech Set) dataset.
- Applied **PCA** to retain 99% variance and **SMOTE** for class balancing.
- Trained multiple classifiers (SVM, Random Forest, KNN, Logistic Regression).
- Enabled real-time emotion inference for affective computing use cases.

### ğŸ§  Workflow
1. **Audio Processing** â†’ MFCC extraction (librosa)  
2. **Dimensionality Reduction** â†’ PCA  
3. **Balancing** â†’ SMOTE  
4. **Modeling** â†’ SVM, RF, KNN, LR  
5. **Deployment Readiness** â†’ Real-time Inference Pipeline  

### ğŸ“‚ Dataset
- ğŸ“¥ [Toronto Emotional Speech Set (TESS) â€“ Kaggle](https://www.kaggle.com/datasets/ejlok1/toronto-emotional-speech-set-tess)

### ğŸ› ï¸ Tech Stack
`Python`, `librosa`, `scikit-learn`, `Pandas`, `Seaborn`

### ğŸ“Š Results
- ğŸ¯ Achieved 91% test accuracy using SVM  
- ğŸ§ Built for real-time audio-based emotion detection
